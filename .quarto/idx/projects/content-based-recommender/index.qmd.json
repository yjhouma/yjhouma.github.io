{"title":"Creating a Content-based Recommender System","markdown":{"yaml":{"title":"Creating a Content-based Recommender System","author":"Yohanes Jhouma Parulian N","date":"2024-06-15","format":{"html":{"toc":true,"toc-location":"right","toc-title":"On this page","toc-depth":3}}},"headingText":"Libraries for this project","containsRefs":false,"markdown":"\n\n\n\n\n\n```{python}\nimport pandas as pd\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport pandas as pd\nimport numpy as np\nimport json\n```\n\n# Introduction\n\nHave you ever had a friend ask you to recommend a movie or song based on something they just watched or listened to? It's pretty common, right? You think about what they might like, considering their taste, and suggest something similar. That's basically what a recommender system does, but with a lot more data and a bit more sophistication.\n\nRecommender systems are like your personal assistant, suggesting things you might like based on your preferences and behavior. They're everywhere: Netflix suggests movies, Spotify recommends songs, and Amazon points you to products you might want to buy.\n\nIn general, there are three types od recommender systems:\n\n1. **Content-Based**. Imagine you just watched a movie with your favorite genre. A content-based system would suggest other movies with similar genres and plot.\n\n2. **Collaborative Filtering**. Think of Netflix or Spotify, where you get recommendations based on what other users with similar tastes liked.\n\n3. **Hybrid**. Like the name suggests, this recommender system combine both content-based and colaborative filtering to give a more accurate suggestions.\n\n\nIn this project, we're building a content-based recommender system using the TMDB 5000 dataset. We'll create a system that can suggest movies based on their content.\n\n# Dataset\n\nFor this project, we're using the TMDB 5000 dataset, which is a collection of movie information gathered from [The Movie Database (TMDB)](https://www.kaggle.com/datasets/tmdb/tmdb-movie-metadata). This dataset provides a wealth of details about movies, including metadata like titles, genres, plot summaries, cast and crew, release dates, and user ratings.\n\n\n\n## Structure of the Dataset\nThe TMDB 5000 dataset consists of two main files:\n\n1. **movies_metadata.csv**: Contains metadata for each movie, including title, genres, overview, release date, and more.\n2. **credits.csv**: Contains information about the cast and crew for each movie.\n\n\n### Movies Metadata\n\n\n```{python}\nmovies_metadata_df = pd.read_csv('data/tmdb_5000_movies.csv')\nmovies_metadata_df.shape\n```\n\n```{python}\nmovies_metadata_df.head(3)\n```\n\n### Movies Credit\n\n```{python}\nmovies_credit_df = pd.read_csv('data/tmdb_5000_credits.csv')\nmovies_credit_df.shape\n```\n\n```{python}\nmovies_credit_df.head(3)\n```\n\n# Data Preprocessing\n\n```{python}\nmovies_metadata_df.info()\n```\n\n```{python}\nmovies_credit_df.info()\n```\n\n### Combining both tables\n\n```{python}\nmovies_df = movies_metadata_df.merge(movies_credit_df.drop('title',axis=1),left_on='id',right_on='movie_id', how='left').drop('movie_id', axis=1)\n```\n\n## Handling missing value\n\nAs can be seen in the previous chunk of code, there are some columns with missing values. Generally, we need to handle these missing values. In this case, we are lucky because most of the missing values are not directly related to the content of the movie. For example, the homepage is unlikely to be a significant factor when recommending a movie.\n\nOn the other hand, the overview column contains a summary of the movie's plot, which is crucial for our recommendation system. Therefore, we need to handle missing values in this column carefully.\n\nFor now, we will ignore missing values in columns that do not directly impact the movie content and drop rows with missing values in crucial columns like overview.\n\nAlthough tagline can contains information crucial for the content, we choose not to drop rows with missing values in the tagline column. The tagline is a more condensed version of the information in the overview, meaning its information is already covered by the overview. Therefore, the absence of a tagline does not significantly impact our ability to recommend movies based on their content. Additionally not all movie have a tagline.\n\n```{python}\nmovies_df = movies_df[~movies_df['overview'].isna()].reset_index(drop=True)\n```\n\n```{python}\nmovies_df.info()\n```\n\n# Exploratory Data Analysis\n\nSince we have a content-based recommender system in mind, we will explore the data that contains information about the content of the movie. Imagine that your friend comes to you and says, \"I've just watched The Dark Knight and I love it. Do you know any other movies like that?\" There are several factors you can consider to give your friend recommendations:\n\n1. **The Genre**. Maybe your friend likes action-packed movies or superhero movies. Considering this, you can recommend other action or superhero movies.\n\n2. **Keywords of the Movie**. This movie might have some keywords attached to it, like DC Comics, superhero, Batman. Considering this, you can recommend not only random superhero movies but something like Superman or Wonder Woman that share the same keywords.\n\n3. **The Plot**. Maybe your friend is interested in movies with similar plots. A movie where the hero has to save a city from a terrorist attack, for example.\n\n4. **The Director**. Each movie director has their own style. Maybe your friend is more interested in the style of Christopher Nolan.\n\n\nConsidering all these factors and many others is the key to making a reliable recommender system.\n\n\n\n## Exploring Genre\n\n### How many Genre a movie have?\n\n```{python}\nmovies_df['genres'].apply(lambda x: len(json.loads(x))).describe()\n```\n\n```{python}\nmovies_df['genres'].apply(lambda x: len(json.loads(x))).value_counts().sort_index().plot.bar()\n```\n\nAs can be seen in the description and the plot, a movie can have a range of 0-7 genres. A movie with 0 genres can be considered as a movie for which we don't know the genre. From the visualization, we can see that most movies have 2 to 3 genres associated with them.\n\n### Genre Distribution\n\n```{python}\ndef create_genre_list(x):\n    genre_movie = []\n    jdata = json.loads(x)\n    for d in jdata:\n        genre_movie.append(d['name'])\n    return genre_movie\n\n\ngenre_series = []\n\nfor i, r in movies_df.iterrows():\n    genre_series += create_genre_list(r['genres'])\n\ngenre_series = pd.Series(genre_series)\n```\n\n```{python}\ngenre_series.value_counts().plot.bar()\n```\n\n```{python}\nlen(genre_series.value_counts())\n```\n\nWe can see there are 20 genres recorded in total. From these 20 genres, the most common one is drama. However, keep in mind that what makes a movie unique is not just a single genre but combinations of genres. For example, an Action-Comedy movie will have a very different tone from an Action-Mystery movie. Therefore, it is important to maintain this connection later when we create our recommender system.\n\n## Exploring Keywords\n\n### Numbers of Keywords in a movie\n\n```{python}\nmovies_df['keywords'].apply(lambda x: len(json.loads(x))).describe()\n```\n\n```{python}\nmovies_df['keywords'].apply(lambda x: len(json.loads(x))).value_counts().sort_index().plot.bar()\n```\n\nUnlike genres, a movie can have a lot more keywords associated with it. In our database, it can range up to 97 keywords, with a median of 6. This means half of the movies have at least 6 keywords attached to them. \n\n## How many keywords there are\n\n```{python}\nkeywords = []\nfor i,r in movies_df.iterrows():\n    jdata = json.loads(r['keywords'])\n    for d in jdata:\n        keywords.append(d['name'])\n\nkeywords = pd.Series(keywords)\n```\n\n```{python}\nlen(keywords.unique())\n```\n\n```{python}\nkeywords.value_counts().describe()\n```\n\nThe total number of keywords available is 9808, however, only a quarter of them appear in at least three different movies. We can keep this in mind, since a keyword that appears only in one or two movies will not provide much information for the recommender system.\n\n## Exploring Overview/The plot description\n\n### How many words contained in an overview?\n\n```{python}\nmovies_df['overview'].apply(lambda x: len(x.split())).describe()\n```\n\n```{python}\nmovies_df['overview'].apply(lambda x: len(x.split())).hist()\n```\n\n```{python}\nmovies_df[movies_df['overview'].apply(lambda x: len(x.split())) == 0]\n```\n\nAs can be seen that the range of overview length is between 0 to 175. Remember that we've drop a movie with no overview. It seems like there are still a movie that don't have an overview. Since it's only one movie i think it is fair to also drop this movie\n\n```{python}\nmovies_df = movies_df[movies_df['overview'].apply(lambda x: len(x.split())) > 0].reset_index(drop=True)\n```\n\n## Exploring Director\n\n### Extracting Director from the Crew\n\n```{python}\ndef return_director(x):\n\n    jdata = json.loads(x)\n    for d in jdata:\n        if d['job'] == 'Director':\n            return d['name']\n    \n    return ''\n```\n\n```{python}\nmovies_df['crew'].apply(return_director).value_counts().describe()\n```\n\n```{python}\nmovies_df['crew'].apply(return_director).value_counts()[:20].plot.bar()\n```\n\n```{python}\nmovies_df['director'] = movies_df['crew'].apply(return_director)\n```\n\nThere are 2346 director recorded in the data, however the majority of them only have one movie directed by them. With the same logic as keywords, we need to keep in mind that the director with only few movies will not enrich our recommender system\n\n# Feature Engineering\n\n## Considering Ranking and Scoring\n\nWhen you recommend something to someone, you usually rank options based on how well they match what the person likes. In recommender systems, we do the same by calculating a score for each item. This score tells us how relevant each item is to the user's interests, and we use these scores to rank the items.\n\nIn a recommender system, feature engineering is all about transforming raw data into something meaningful that can be used to make recommendations. Considering scoring in recommender systems, we need to transform all the content we have into something that can be quantified. In other words, we have to extract the features into vectors that we can calculate the similarities.\n\n## Genres\n\nFor our genres, we want to make a vector representation that considers that a movie can have more than one genre. Since there are only 20 genres available, it is reasonable for us to use a binary vector.\n\nFor example, if there are three genres only: Action, Drama, and Comedy:\n* A movie with Action-Drama genres will be represented with 1 1 0.\n* A movie with Drama-Comedy genres will be represented with 0 1 1.\n\n```{python}\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n\ndef extract_genre(X):\n    genre_matrix = []\n    for d in X:\n        row_genres = []\n        jdata = json.loads(d)\n        for j in jdata:\n            row_genres.append(j['name'])\n        genre_matrix.append(row_genres)\n\n    return genre_matrix\n\ngenre_extractor = FunctionTransformer(extract_genre)\n```\n\n```{python}\ngenre_vectorizer = CountVectorizer(analyzer= lambda x: x, binary=True)\n\ngenre_matrix = genre_vectorizer.fit_transform(genre_extractor.fit_transform(movies_df['genres'])).toarray()\n```\n\n```{python}\ngenre_matrix\n```\n\n## Keywords\n\nWe're using TF-IDF (Term Frequency-Inverse Document Frequency) to convert keywords into vectors. TF-IDF helps capture the importance of each keyword by balancing how often a keyword appears in a movie against how common it is across all movies. This way, unique keywords for specific movies get more weight, while common ones get less.\n\nSince we know that most keywords appear only once or twice, we won't need to map those keywords to the vector. This can reduce our dimensionality significantly without losing much information. For now, we will map only keywords that appear at least 5 times.\n\n\n\n\n```{python}\ndef extract_keywords(X):\n    keywords_matrix = []\n    for d in X:\n        row_keywords = []\n        jdata = json.loads(d)\n        for j in jdata:\n            row_keywords.append(j['name'])\n        keywords_matrix.append(row_keywords)\n\n    return keywords_matrix\n\nkeywords_extractor = FunctionTransformer(extract_keywords)\n```\n\n```{python}\nkeywords_vectorizer = TfidfVectorizer(analyzer= lambda x: x, min_df=5, binary=True)\nkeywords_matrix = keywords_vectorizer.fit_transform(keywords_extractor.fit_transform(movies_df['keywords'])).toarray()\n```\n\n```{python}\nkeywords_matrix\n```\n\n## Overview\n\nFor plot overviews, we will use Sentence-BERT (SBERT) to create embeddings. Sentence Transformers, based on the SBERT architecture, allow us to convert text into high-dimensional vectors that capture the semantic meaning of the text.\n\nUnlike simple TF-IDF, SBERT captures the deeper semantic meaning of the text, understanding the context and relationships between words. SBERT is particularly effective for longer texts like plot overviews, providing a more nuanced representation than TF-IDF.\nUsing SBERT embeddings for plot overviews helps us create rich, meaningful representations of the movie plots, enhancing the quality of our recommendations.\n\n\n\n```{python}\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sentence_transformers import SentenceTransformer\n\n\nclass SbertTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self, model_name=\"all-MiniLM-L6-v2\") -> None:\n        self.model_name =model_name\n        self.model = SentenceTransformer(model_name)\n    \n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        return self.model.encode(X.str.lower())\n```\n\n```{python}\noverview_embedder = SbertTransformer() # Using small model\noverview_embeddings = overview_embedder.fit_transform(movies_df['overview'])\n```\n\n```{python}\noverview_embeddings.shape\n```\n\n## Director\n\nWe can use similar logic as keywords and genres to vectorize director information. Since director with a few movie wont help us much in term of recommendation, we will use only director with more than 4 movie to vectorize. Due to the fact a movie can only have one director we will be usinng binary representation instead of tfidf\n\n```{python}\nmovies_df['director']\n```\n\n```{python}\ndirector_vectorizer = CountVectorizer(analyzer= lambda x: [x] if x != '' else [], binary=True, min_df=5)\n\n\ndirector_matrix = director_vectorizer.fit_transform(movies_df['director']).toarray()\n```\n\n```{python}\ndirector_matrix.shape\n```\n\n## Putting it Together\n\n```{python}\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\n```\n\n### Creating Pipeline for Feature Extraction\n\n```{python}\n# Pipeline for Genre Feature Extraction\n\ngenre_pipeline = Pipeline([\n    ('genre_extractor', FunctionTransformer(extract_genre)),\n    ('genre_vectorizer', CountVectorizer(analyzer= lambda x: x, binary=True))\n])\n\n\n# Pipeline for Keywords Feature Extraction\n\nkeywords_pipeline = Pipeline([\n    ('keywords_extractor', FunctionTransformer(extract_keywords)),\n    ('keywords_vectorizer', TfidfVectorizer(analyzer= lambda x: x, min_df=5, binary=True))\n])\n\n```\n\n### Joining all feature\n\n```{python}\nfeature_extractor = ColumnTransformer([\n    ('genres_pipeline', genre_pipeline, \"genres\"),\n    ('keywords_pipeline', keywords_pipeline, \"keywords\"),\n    ('overview_pipeline', SbertTransformer(), \"overview\"),\n    ('director_pipeline', CountVectorizer(analyzer= lambda x: [x] if x != '' else [], binary=True, min_df=5), \"director\")\n])\n```\n\n```{python}\nfeature_matrix = feature_extractor.fit_transform(movies_df).toarray()\n```\n\n```{python}\nfeature_matrix\n```\n\n# Build a Recommender System\n\nRemember that I mentioned about scoring and ranking. Now in order to actually recommend a movie, we need to score their similirarity to each other. There are several ways to do this, but the most common one for data involving text is **Cosine Similarity**.\n\nCosine similarity measures how similar two items are by looking at the angle between their vectors. Think of it as comparing the direction of two arrows, regardless of their length. If the arrows point in the same direction, they're similar. In addition to that, It's quick and can handle lots of data without much fuss.\n\n```{python}\nmovies_list = movies_df[['id','original_title']].copy()\n```\n\n```{python}\nsimilarity_matrix = cosine_similarity(feature_matrix, feature_matrix)\n```\n\n```{python}\ndef find_similar_movie_using_similarity(movie_id: int):\n    indx = movies_list[movies_list['id'] == movie_id].index.tolist()[0]\n    top_similar = sorted(list(enumerate(similarity_matrix[indx])), key=lambda x: x[1], reverse=True)[1:11]\n    recommended_movies = movies_list.iloc[[t[0] for t in top_similar]].copy()\n    recommended_movies['similarity_score'] = [t[1] for t in top_similar]\n    return recommended_movies\n```\n\n## Running the Recommender Function\n\nNow let's use this recommender system to recommend movies to our friend who just watched Batman Begins.\n\n```{python}\nmovies_list[movies_list['original_title'].str.contains('Batman')]\n```\n\n```{python}\nbatman_begins_id = 272\n```\n\n```{python}\nfind_similar_movie_using_similarity(batman_begins_id)\n```\n\nAs can be seen from our algorithm, when asked to recommend movies like \"Batman Begins,\" it recommends \"The Dark Knight\" and \"The Dark Knight Rises,\" which are sequels to the movie. On top of that, we also see movies with some detective and superhero vibes. This indicates that our content-based recommender system is effectively identifying and recommending movies that share significant characteristics with the user's input, thus providing relevant and meaningful suggestions.\n\n# Improving Recomender System\n\nCan we improve this model? Yes. In order to improve a model, we need a strategy to quantify the performance of the model itself. In terms of recommender systems, one common evaluation metric is Precision at K (P@K). Precision at K measures the relevance of the top K recommendations made by the system. It is defined as the proportion of recommended items in the top K set that are relevant.\n\nUnfortunately, our dataset is not ideal for evaluating the recommender system since we don't have the actual user consumption data. Without knowing what movies users have actually watched and liked, we can't accurately measure how well our recommendations match user preferences.\n\nThat being said, there are several strategies to improve the model:\n1. **Give Different Weight to Each Feature:** Currently, we are treating every feature as equally important. Depending on the case, this might not be the most optimal. By adjusting the weights of different features (e.g., giving more importance to plot similarity over genre similarity), we can potentially improve the relevance of our recommendations.\n    \n2. **Add More Relevant Features:** We can incorporate additional features that are relevant to the content of the movie, such as the cast, the director, or even more granular details like the sound engineer. These additional features can provide a richer context for making recommendations.\n\n3. **Combine Content-Based Approach with Collaborative Filtering:** Creating a hybrid recommender system by combining content-based filtering with collaborative filtering can leverage the strengths of both approaches. While content-based filtering focuses on item attributes, collaborative filtering leverages user behavior and preferences. This combination can result in more accurate and personalized recommendations.\n\n# Conclusion\n\n\nBuilding a content-based recommender system using the TMDB 5000 dataset has allowed us to explore various aspects of feature engineering and similarity scoring. By converting genres into binary vectors, using TF-IDF for keywords, and employing SBERT for plot overviews, we created a comprehensive model capable of providing relevant movie recommendations.\n\nOur model, when asked to recommend movies like \"Batman Begins,\" successfully suggested sequels like \"The Dark Knight\" and \"The Dark Knight Rises,\" as well as other movies with similar detective and superhero vibes. This demonstrates the effectiveness of our content-based approach in capturing the nuances of movie content.\n\nHowever, there is always room for improvement. Precision at K (P@K) is a common metric for evaluating recommender systems, but our dataset lacks actual user consumption data, which limits our ability to accurately measure performance.\n\n","srcMarkdownNoYaml":"\n\n\n\n# Libraries for this project\n\n\n```{python}\nimport pandas as pd\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport pandas as pd\nimport numpy as np\nimport json\n```\n\n# Introduction\n\nHave you ever had a friend ask you to recommend a movie or song based on something they just watched or listened to? It's pretty common, right? You think about what they might like, considering their taste, and suggest something similar. That's basically what a recommender system does, but with a lot more data and a bit more sophistication.\n\nRecommender systems are like your personal assistant, suggesting things you might like based on your preferences and behavior. They're everywhere: Netflix suggests movies, Spotify recommends songs, and Amazon points you to products you might want to buy.\n\nIn general, there are three types od recommender systems:\n\n1. **Content-Based**. Imagine you just watched a movie with your favorite genre. A content-based system would suggest other movies with similar genres and plot.\n\n2. **Collaborative Filtering**. Think of Netflix or Spotify, where you get recommendations based on what other users with similar tastes liked.\n\n3. **Hybrid**. Like the name suggests, this recommender system combine both content-based and colaborative filtering to give a more accurate suggestions.\n\n\nIn this project, we're building a content-based recommender system using the TMDB 5000 dataset. We'll create a system that can suggest movies based on their content.\n\n# Dataset\n\nFor this project, we're using the TMDB 5000 dataset, which is a collection of movie information gathered from [The Movie Database (TMDB)](https://www.kaggle.com/datasets/tmdb/tmdb-movie-metadata). This dataset provides a wealth of details about movies, including metadata like titles, genres, plot summaries, cast and crew, release dates, and user ratings.\n\n\n\n## Structure of the Dataset\nThe TMDB 5000 dataset consists of two main files:\n\n1. **movies_metadata.csv**: Contains metadata for each movie, including title, genres, overview, release date, and more.\n2. **credits.csv**: Contains information about the cast and crew for each movie.\n\n\n### Movies Metadata\n\n\n```{python}\nmovies_metadata_df = pd.read_csv('data/tmdb_5000_movies.csv')\nmovies_metadata_df.shape\n```\n\n```{python}\nmovies_metadata_df.head(3)\n```\n\n### Movies Credit\n\n```{python}\nmovies_credit_df = pd.read_csv('data/tmdb_5000_credits.csv')\nmovies_credit_df.shape\n```\n\n```{python}\nmovies_credit_df.head(3)\n```\n\n# Data Preprocessing\n\n```{python}\nmovies_metadata_df.info()\n```\n\n```{python}\nmovies_credit_df.info()\n```\n\n### Combining both tables\n\n```{python}\nmovies_df = movies_metadata_df.merge(movies_credit_df.drop('title',axis=1),left_on='id',right_on='movie_id', how='left').drop('movie_id', axis=1)\n```\n\n## Handling missing value\n\nAs can be seen in the previous chunk of code, there are some columns with missing values. Generally, we need to handle these missing values. In this case, we are lucky because most of the missing values are not directly related to the content of the movie. For example, the homepage is unlikely to be a significant factor when recommending a movie.\n\nOn the other hand, the overview column contains a summary of the movie's plot, which is crucial for our recommendation system. Therefore, we need to handle missing values in this column carefully.\n\nFor now, we will ignore missing values in columns that do not directly impact the movie content and drop rows with missing values in crucial columns like overview.\n\nAlthough tagline can contains information crucial for the content, we choose not to drop rows with missing values in the tagline column. The tagline is a more condensed version of the information in the overview, meaning its information is already covered by the overview. Therefore, the absence of a tagline does not significantly impact our ability to recommend movies based on their content. Additionally not all movie have a tagline.\n\n```{python}\nmovies_df = movies_df[~movies_df['overview'].isna()].reset_index(drop=True)\n```\n\n```{python}\nmovies_df.info()\n```\n\n# Exploratory Data Analysis\n\nSince we have a content-based recommender system in mind, we will explore the data that contains information about the content of the movie. Imagine that your friend comes to you and says, \"I've just watched The Dark Knight and I love it. Do you know any other movies like that?\" There are several factors you can consider to give your friend recommendations:\n\n1. **The Genre**. Maybe your friend likes action-packed movies or superhero movies. Considering this, you can recommend other action or superhero movies.\n\n2. **Keywords of the Movie**. This movie might have some keywords attached to it, like DC Comics, superhero, Batman. Considering this, you can recommend not only random superhero movies but something like Superman or Wonder Woman that share the same keywords.\n\n3. **The Plot**. Maybe your friend is interested in movies with similar plots. A movie where the hero has to save a city from a terrorist attack, for example.\n\n4. **The Director**. Each movie director has their own style. Maybe your friend is more interested in the style of Christopher Nolan.\n\n\nConsidering all these factors and many others is the key to making a reliable recommender system.\n\n\n\n## Exploring Genre\n\n### How many Genre a movie have?\n\n```{python}\nmovies_df['genres'].apply(lambda x: len(json.loads(x))).describe()\n```\n\n```{python}\nmovies_df['genres'].apply(lambda x: len(json.loads(x))).value_counts().sort_index().plot.bar()\n```\n\nAs can be seen in the description and the plot, a movie can have a range of 0-7 genres. A movie with 0 genres can be considered as a movie for which we don't know the genre. From the visualization, we can see that most movies have 2 to 3 genres associated with them.\n\n### Genre Distribution\n\n```{python}\ndef create_genre_list(x):\n    genre_movie = []\n    jdata = json.loads(x)\n    for d in jdata:\n        genre_movie.append(d['name'])\n    return genre_movie\n\n\ngenre_series = []\n\nfor i, r in movies_df.iterrows():\n    genre_series += create_genre_list(r['genres'])\n\ngenre_series = pd.Series(genre_series)\n```\n\n```{python}\ngenre_series.value_counts().plot.bar()\n```\n\n```{python}\nlen(genre_series.value_counts())\n```\n\nWe can see there are 20 genres recorded in total. From these 20 genres, the most common one is drama. However, keep in mind that what makes a movie unique is not just a single genre but combinations of genres. For example, an Action-Comedy movie will have a very different tone from an Action-Mystery movie. Therefore, it is important to maintain this connection later when we create our recommender system.\n\n## Exploring Keywords\n\n### Numbers of Keywords in a movie\n\n```{python}\nmovies_df['keywords'].apply(lambda x: len(json.loads(x))).describe()\n```\n\n```{python}\nmovies_df['keywords'].apply(lambda x: len(json.loads(x))).value_counts().sort_index().plot.bar()\n```\n\nUnlike genres, a movie can have a lot more keywords associated with it. In our database, it can range up to 97 keywords, with a median of 6. This means half of the movies have at least 6 keywords attached to them. \n\n## How many keywords there are\n\n```{python}\nkeywords = []\nfor i,r in movies_df.iterrows():\n    jdata = json.loads(r['keywords'])\n    for d in jdata:\n        keywords.append(d['name'])\n\nkeywords = pd.Series(keywords)\n```\n\n```{python}\nlen(keywords.unique())\n```\n\n```{python}\nkeywords.value_counts().describe()\n```\n\nThe total number of keywords available is 9808, however, only a quarter of them appear in at least three different movies. We can keep this in mind, since a keyword that appears only in one or two movies will not provide much information for the recommender system.\n\n## Exploring Overview/The plot description\n\n### How many words contained in an overview?\n\n```{python}\nmovies_df['overview'].apply(lambda x: len(x.split())).describe()\n```\n\n```{python}\nmovies_df['overview'].apply(lambda x: len(x.split())).hist()\n```\n\n```{python}\nmovies_df[movies_df['overview'].apply(lambda x: len(x.split())) == 0]\n```\n\nAs can be seen that the range of overview length is between 0 to 175. Remember that we've drop a movie with no overview. It seems like there are still a movie that don't have an overview. Since it's only one movie i think it is fair to also drop this movie\n\n```{python}\nmovies_df = movies_df[movies_df['overview'].apply(lambda x: len(x.split())) > 0].reset_index(drop=True)\n```\n\n## Exploring Director\n\n### Extracting Director from the Crew\n\n```{python}\ndef return_director(x):\n\n    jdata = json.loads(x)\n    for d in jdata:\n        if d['job'] == 'Director':\n            return d['name']\n    \n    return ''\n```\n\n```{python}\nmovies_df['crew'].apply(return_director).value_counts().describe()\n```\n\n```{python}\nmovies_df['crew'].apply(return_director).value_counts()[:20].plot.bar()\n```\n\n```{python}\nmovies_df['director'] = movies_df['crew'].apply(return_director)\n```\n\nThere are 2346 director recorded in the data, however the majority of them only have one movie directed by them. With the same logic as keywords, we need to keep in mind that the director with only few movies will not enrich our recommender system\n\n# Feature Engineering\n\n## Considering Ranking and Scoring\n\nWhen you recommend something to someone, you usually rank options based on how well they match what the person likes. In recommender systems, we do the same by calculating a score for each item. This score tells us how relevant each item is to the user's interests, and we use these scores to rank the items.\n\nIn a recommender system, feature engineering is all about transforming raw data into something meaningful that can be used to make recommendations. Considering scoring in recommender systems, we need to transform all the content we have into something that can be quantified. In other words, we have to extract the features into vectors that we can calculate the similarities.\n\n## Genres\n\nFor our genres, we want to make a vector representation that considers that a movie can have more than one genre. Since there are only 20 genres available, it is reasonable for us to use a binary vector.\n\nFor example, if there are three genres only: Action, Drama, and Comedy:\n* A movie with Action-Drama genres will be represented with 1 1 0.\n* A movie with Drama-Comedy genres will be represented with 0 1 1.\n\n```{python}\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n\ndef extract_genre(X):\n    genre_matrix = []\n    for d in X:\n        row_genres = []\n        jdata = json.loads(d)\n        for j in jdata:\n            row_genres.append(j['name'])\n        genre_matrix.append(row_genres)\n\n    return genre_matrix\n\ngenre_extractor = FunctionTransformer(extract_genre)\n```\n\n```{python}\ngenre_vectorizer = CountVectorizer(analyzer= lambda x: x, binary=True)\n\ngenre_matrix = genre_vectorizer.fit_transform(genre_extractor.fit_transform(movies_df['genres'])).toarray()\n```\n\n```{python}\ngenre_matrix\n```\n\n## Keywords\n\nWe're using TF-IDF (Term Frequency-Inverse Document Frequency) to convert keywords into vectors. TF-IDF helps capture the importance of each keyword by balancing how often a keyword appears in a movie against how common it is across all movies. This way, unique keywords for specific movies get more weight, while common ones get less.\n\nSince we know that most keywords appear only once or twice, we won't need to map those keywords to the vector. This can reduce our dimensionality significantly without losing much information. For now, we will map only keywords that appear at least 5 times.\n\n\n\n\n```{python}\ndef extract_keywords(X):\n    keywords_matrix = []\n    for d in X:\n        row_keywords = []\n        jdata = json.loads(d)\n        for j in jdata:\n            row_keywords.append(j['name'])\n        keywords_matrix.append(row_keywords)\n\n    return keywords_matrix\n\nkeywords_extractor = FunctionTransformer(extract_keywords)\n```\n\n```{python}\nkeywords_vectorizer = TfidfVectorizer(analyzer= lambda x: x, min_df=5, binary=True)\nkeywords_matrix = keywords_vectorizer.fit_transform(keywords_extractor.fit_transform(movies_df['keywords'])).toarray()\n```\n\n```{python}\nkeywords_matrix\n```\n\n## Overview\n\nFor plot overviews, we will use Sentence-BERT (SBERT) to create embeddings. Sentence Transformers, based on the SBERT architecture, allow us to convert text into high-dimensional vectors that capture the semantic meaning of the text.\n\nUnlike simple TF-IDF, SBERT captures the deeper semantic meaning of the text, understanding the context and relationships between words. SBERT is particularly effective for longer texts like plot overviews, providing a more nuanced representation than TF-IDF.\nUsing SBERT embeddings for plot overviews helps us create rich, meaningful representations of the movie plots, enhancing the quality of our recommendations.\n\n\n\n```{python}\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sentence_transformers import SentenceTransformer\n\n\nclass SbertTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self, model_name=\"all-MiniLM-L6-v2\") -> None:\n        self.model_name =model_name\n        self.model = SentenceTransformer(model_name)\n    \n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        return self.model.encode(X.str.lower())\n```\n\n```{python}\noverview_embedder = SbertTransformer() # Using small model\noverview_embeddings = overview_embedder.fit_transform(movies_df['overview'])\n```\n\n```{python}\noverview_embeddings.shape\n```\n\n## Director\n\nWe can use similar logic as keywords and genres to vectorize director information. Since director with a few movie wont help us much in term of recommendation, we will use only director with more than 4 movie to vectorize. Due to the fact a movie can only have one director we will be usinng binary representation instead of tfidf\n\n```{python}\nmovies_df['director']\n```\n\n```{python}\ndirector_vectorizer = CountVectorizer(analyzer= lambda x: [x] if x != '' else [], binary=True, min_df=5)\n\n\ndirector_matrix = director_vectorizer.fit_transform(movies_df['director']).toarray()\n```\n\n```{python}\ndirector_matrix.shape\n```\n\n## Putting it Together\n\n```{python}\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\n```\n\n### Creating Pipeline for Feature Extraction\n\n```{python}\n# Pipeline for Genre Feature Extraction\n\ngenre_pipeline = Pipeline([\n    ('genre_extractor', FunctionTransformer(extract_genre)),\n    ('genre_vectorizer', CountVectorizer(analyzer= lambda x: x, binary=True))\n])\n\n\n# Pipeline for Keywords Feature Extraction\n\nkeywords_pipeline = Pipeline([\n    ('keywords_extractor', FunctionTransformer(extract_keywords)),\n    ('keywords_vectorizer', TfidfVectorizer(analyzer= lambda x: x, min_df=5, binary=True))\n])\n\n```\n\n### Joining all feature\n\n```{python}\nfeature_extractor = ColumnTransformer([\n    ('genres_pipeline', genre_pipeline, \"genres\"),\n    ('keywords_pipeline', keywords_pipeline, \"keywords\"),\n    ('overview_pipeline', SbertTransformer(), \"overview\"),\n    ('director_pipeline', CountVectorizer(analyzer= lambda x: [x] if x != '' else [], binary=True, min_df=5), \"director\")\n])\n```\n\n```{python}\nfeature_matrix = feature_extractor.fit_transform(movies_df).toarray()\n```\n\n```{python}\nfeature_matrix\n```\n\n# Build a Recommender System\n\nRemember that I mentioned about scoring and ranking. Now in order to actually recommend a movie, we need to score their similirarity to each other. There are several ways to do this, but the most common one for data involving text is **Cosine Similarity**.\n\nCosine similarity measures how similar two items are by looking at the angle between their vectors. Think of it as comparing the direction of two arrows, regardless of their length. If the arrows point in the same direction, they're similar. In addition to that, It's quick and can handle lots of data without much fuss.\n\n```{python}\nmovies_list = movies_df[['id','original_title']].copy()\n```\n\n```{python}\nsimilarity_matrix = cosine_similarity(feature_matrix, feature_matrix)\n```\n\n```{python}\ndef find_similar_movie_using_similarity(movie_id: int):\n    indx = movies_list[movies_list['id'] == movie_id].index.tolist()[0]\n    top_similar = sorted(list(enumerate(similarity_matrix[indx])), key=lambda x: x[1], reverse=True)[1:11]\n    recommended_movies = movies_list.iloc[[t[0] for t in top_similar]].copy()\n    recommended_movies['similarity_score'] = [t[1] for t in top_similar]\n    return recommended_movies\n```\n\n## Running the Recommender Function\n\nNow let's use this recommender system to recommend movies to our friend who just watched Batman Begins.\n\n```{python}\nmovies_list[movies_list['original_title'].str.contains('Batman')]\n```\n\n```{python}\nbatman_begins_id = 272\n```\n\n```{python}\nfind_similar_movie_using_similarity(batman_begins_id)\n```\n\nAs can be seen from our algorithm, when asked to recommend movies like \"Batman Begins,\" it recommends \"The Dark Knight\" and \"The Dark Knight Rises,\" which are sequels to the movie. On top of that, we also see movies with some detective and superhero vibes. This indicates that our content-based recommender system is effectively identifying and recommending movies that share significant characteristics with the user's input, thus providing relevant and meaningful suggestions.\n\n# Improving Recomender System\n\nCan we improve this model? Yes. In order to improve a model, we need a strategy to quantify the performance of the model itself. In terms of recommender systems, one common evaluation metric is Precision at K (P@K). Precision at K measures the relevance of the top K recommendations made by the system. It is defined as the proportion of recommended items in the top K set that are relevant.\n\nUnfortunately, our dataset is not ideal for evaluating the recommender system since we don't have the actual user consumption data. Without knowing what movies users have actually watched and liked, we can't accurately measure how well our recommendations match user preferences.\n\nThat being said, there are several strategies to improve the model:\n1. **Give Different Weight to Each Feature:** Currently, we are treating every feature as equally important. Depending on the case, this might not be the most optimal. By adjusting the weights of different features (e.g., giving more importance to plot similarity over genre similarity), we can potentially improve the relevance of our recommendations.\n    \n2. **Add More Relevant Features:** We can incorporate additional features that are relevant to the content of the movie, such as the cast, the director, or even more granular details like the sound engineer. These additional features can provide a richer context for making recommendations.\n\n3. **Combine Content-Based Approach with Collaborative Filtering:** Creating a hybrid recommender system by combining content-based filtering with collaborative filtering can leverage the strengths of both approaches. While content-based filtering focuses on item attributes, collaborative filtering leverages user behavior and preferences. This combination can result in more accurate and personalized recommendations.\n\n# Conclusion\n\n\nBuilding a content-based recommender system using the TMDB 5000 dataset has allowed us to explore various aspects of feature engineering and similarity scoring. By converting genres into binary vectors, using TF-IDF for keywords, and employing SBERT for plot overviews, we created a comprehensive model capable of providing relevant movie recommendations.\n\nOur model, when asked to recommend movies like \"Batman Begins,\" successfully suggested sequels like \"The Dark Knight\" and \"The Dark Knight Rises,\" as well as other movies with similar detective and superhero vibes. This demonstrates the effectiveness of our content-based approach in capturing the nuances of movie content.\n\nHowever, there is always room for improvement. Precision at K (P@K) is a common metric for evaluating recommender systems, but our dataset lacks actual user consumption data, which limits our ability to accurately measure performance.\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":"auto","echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"toc":true,"toc-depth":3,"output-file":"index.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.5.56","jupyter":{"kernelspec":{"name":"forecasting","language":"python","display_name":"Forecasting Environment"},"conda":{"path":"/opt/homebrew/Caskroom/miniforge/base/bin/conda"},"python":"/opt/homebrew/Caskroom/miniforge/base/envs/forecasting/bin/python"},"theme":"minty","title-block-banner":true,"title":"Creating a Content-based Recommender System","author":"Yohanes Jhouma Parulian N","date":"2024-06-15","toc-location":"right","toc-title":"On this page"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}